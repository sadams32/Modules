---
title: "Module8"
format: html
editor: visual
---

```{r}
install.packages("manipulate")
```

# Notes

Population

:   includes all of the elements from a set of data = N

Sample

:   one or more observations from a population = n

Parameter

:   a measurable characteristic of a population

Statistic

:   a measurable characteristic about a sample

When we do **statistical inference** we are basically trying to draw conclusions about a population based on measurements from a noisy sample or trying to evaluate whether it is reasonable to assume that our sample is drawn from a particular population.

# Probability

### Example: Rolling a Die

We will use the {manipulate} package and the sample() function to explore the effects of sample size on estimates of the probability of different outcomes. The probability of each outcome (rolling a ‚Äú1‚Äù, ‚Äú2‚Äù,‚Ä¶, ‚Äú6‚Äù) is 1 in 6, but our estimate of the probability of each possible outcome will change with sample size.

NOTE: To run {manipulate} effectively, you must run the code directly in the console (i.e., copy and paste the code into the console; using the ‚Äògreen arrow‚Äô or ‚ÄòRun‚Äô buttons on a {manipulate} chunk won‚Äôt have the desired effect). When you run the code as pasted into the console, the graph will appear in the Plots tab of your lower right RStudio pane. Once there, look to the upper left corner of the graph and you should see a small image of a gear. Click on the gear image and a slider will appear, allowing you to manipulate/toggle the sample size (n) of die rolls used to generate the graph.

In this particular graph, you should be able to see the difference in random sampling distribution of die roll outcomes for a given sample size, and how they change as sample size changes.

```{r}
library(manipulate)
outcomes <- c(1, 2, 3, 4, 5, 6)
manipulate(hist(sample(outcomes, n, replace = TRUE), breaks = c(0.5, 1.5, 2.5,
    3.5, 4.5, 5.5, 6.5), probability = TRUE, main = paste("Histogram of Outcomes of ",
    n, " Die Rolls", sep = ""), xlab = "roll", ylab = "probability"), n = slider(0,
    10000, initial = 10, step = 10))
```

## Challenge 1

Write a function to simulate rolling a die where you pass the number of rolls as an argument. Then, use your function to simulate rolling two dice 1000 times and take the sum of the rolls. Plot a histogram of those results.

```{r}
nrolls <- 1000
roll <- function(x) {
    sample(1:6, x, replace = TRUE) # size = x = the number of elements you want to randomly sample from 1:6.
}
two_dice <- roll(nrolls) + roll(nrolls)
hist(two_dice, breaks = c(1.5:12.5), probability = TRUE, main = "Rolling Two Dice",
    xlab = "sum of rolls", ylab = "probability")
```

## Challenge 2

You have a deck of 52 cards, Ace to 10 + 3 face cards in each suit. You draw a card at random.

```{r}
ndraw <- 10000
draw <- function(x) {
  sample(deck, x, replace = TRUE)
}
```

What is the probability that you draw a face card?

```{r}
deck <- c(rep("Face Card", 12), rep("Other", 40))
draws1 <- draw(ndraw)
prob1 = mean(draws1 == "Face Card")
prob1
```

What is the probability that you draw a King?

```{r}
deck <- c(rep("King", 4), rep("Other", 48))
draws2 <- draw(ndraw)
prob2 = mean(draws2 == "King")
prob2
```

What is the probability that you draw a spade?

```{r}
deck <- c(rep("Spade", 13), rep("Other",39))
draws3 <- draw(ndraw)
prob3 = mean(draws3 == "Spade")
prob3
```

What is the probability that you draw a spade given that you draw a face card?

What is the probability that you draw a King given that you draw a face card?

What is the probability that you draw a card that is both from a red suit (hearts or diamonds) and a face card?

# Random Variables

A random variable is a variable whose outcomes are assumed to arise by chance or according to some random or stochastic mechanism. The chances of observing a specific outcome or an outcome value within a specific interval has associated with it a probability.

A probability function is a mathematical function that describes the chance associated with a random variable having a particular outcome or falling within a given range of outcome values.

We can also distinguish two types of probability functions:

(1) **Probability Mass Functions (PMFs)** are associated with discrete random variables. These functions describe the probability that a random variable takes a particular discrete value.

To be a valid PMF, a function must satisfy the following

:   There are ùëòdistinct outcomes ùë•1,ùë•2,...,ùë•ùëò

:   Pr(X=xi) is greater than 0 and less than 1 for all xi

:   Sum of Pr(X = xi) for all x from x1 to xk = 1

### Flipping a Fair Coin

```{r}
outcomes <- c("heads", "tails")
prob <- c(1/2, 1/2)
barplot(prob, ylim = c(0, 0.6), names.arg = outcomes, space = 0.1, xlab =
          "outcome", ylab = "Pr(X = outcome)", main = "Probability Mass Function")
```

Cumulative Probability

```{r}
cumprob <- cumsum(prob)
barplot(cumprob, names.arg = outcomes, space = 0.1, xlab = "outcome", ylab = "Cumulative Pr(X)", main = "Cumulative Probability")
```

### Rolling a Fair Die

```{r}
outcomes <- c(1, 2, 3, 4, 5, 6)
prob <- c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)
barplot(prob, ylim = c(0, 0.5), names.arg = outcomes, space = 0.1, xlab = "outcome", ylab = "Pr(X = outcome)", main = "Probability Mass Function")
```

Cumulative Probability

```{r}
cumprob <- cumsum(prob)
barplot(cumprob, names.arg = outcomes, space = 0.1, xlab = "outcome", ylab = "Cumulative Pr(X)", main = "Cumulative Probability")
```

(2) **Probability Density Functions (PDFs)** are associated with continuous random variables. These functions describe the probability that a random variable falls within a given range of outcome values. The probability associated with that range equals the area under the density function for that range.

To be a valid PDF, a function must satisfy the following:
: The function is non-negative (greater than 0) for any given x
: The total area under the function = 1

### Example: Beta Distribution

The Beta Distribution refers to a family of continuous probability distributions defined over the interval [0, 1] and parametrized by two positive shape parameters, denoted by alpha (a) and beta (b), that appear as exponents of the random variable x and control the shape of the distribution.
 
f(x) = K * x^(a-1) *(1-x^(b - 1)) 

In this example, set K = 2, a = 2, and b = 1 and restrict domain of x to [0,1]

```{r}
library(ggplot2)
a <- 2
b <- 1
K <- 2
x <- seq(from = 0, to = 1, by = 0.025)
fx <- K * x^(a-1) * (1-x)^(b-1)
lower_x <- seq(from = -0.25, to = 0, by = 0.025)  # add some values of x less than zero
upper_x <- seq(from = 1, to = 1.25, by = 0.025)  # add some values of x greater than one
lower_fx <- rep(0, 11)  # add fx=0 values to x<0
upper_fx <- rep(0, 11)  # add fx=0 values to x>1
x <- c(lower_x, x, upper_x)  # paste xs together
fx <- c(lower_fx, fx, upper_fx)  # paste fxs together
d <- as.data.frame(cbind(x, fx))
p <- ggplot(data = d, aes(x = x, y = fx)) + xlab("x") + ylab("f(x)") + geom_line()
p
```

Show the above is a PDF

```{r}
library(manipulate)
manipulate(ggplot(data = d, aes(x = x, y = fx)) + xlab("x") + ylab("f(x)") +
    geom_line() + geom_polygon(data = data.frame(xvals = c(0, n, n, 0), fxvals = c(0,
    K * n^(a - 1) * (1 - n)^(b - 1), 0, 0)), aes(x = xvals, y = fxvals)) + ggtitle(paste("Area Under Function = ",
    0.5 * n * K * n^(a - 1) * (1 - n)^(b - 1), sep = " ")), n = slider(0, 1,
    initial = 0.5, step = 0.01))
```

The shaded area here represents the cumulative probability integrated across f(x) from ‚àíinf to x.

The **cumulative distribution function**, or CDF, of a random variable is defined as the probability of observing a random variable X taking the value of x or less, i.e. F(x) = Pr(X is less than or equal to x)
: This definition applies regardless of whether X is discrete or continuous. Note here we are using F(x) for the cumulative distribution function rather than f(x), which we use for the probability density function. For a continuous variable, the PDF is simply the first derivative of the CDF

```{r}
x <- seq(from = 0, to = 1, by = 0.005)
prob <- 0.5 * x * K * x^(a - 1) * (1 - x)^(b - 1)
barplot(prob, names.arg = x, space = 0, main = "Cumulative Probability", xlab = "x",ylab = "Pr(X ‚â§ x)")
```

The built in R function for the Beta Distribution, pbeta(), can give us the cumulative probability directly, if we specify the values of a = 2 and b = 1.
 
```{r}
pbeta(0.75, 2, 1)  # cumulative probability for x ‚â§ 0.75
pbeta(0.5, 2, 1)  # cumulative probability for x ‚â§ 0.50
```

In general, we find the cumulative probability for a continuous random variable by calculating the area under the probability density function of interest from -inf to x. This is what is is being returned from pbeta(). The other related Beta Distribution functions, e.g., rbeta(), dbeta(), and qbeta(), are also useful. rbeta() draws random observations from a specfied beta distribution. dbeta() gives the point estimate of the beta density function at the value of the argument x, and qbeta() is essentially the converse of pbeta(), i.e., it tells you the value of x that is associated with a particular cumulative probability, or quantile, of the cumulative distribution function. Other PMFs and PDFs have comparable r, d, p, and q functions.

```{r}
pbeta(0.7, 2, 1)  # yields .49
qbeta(0.49, 2, 1)  # yield 0.7
```

We can define the survival function for a random variable X as S(x) = P(X > x) = 1 - P (X <= X) = 1 - f(x)

Finally, we can define the ‚Äúqth‚Äù" quantile of a cumulative distibution function as the value of x at which the CDF has the value ‚Äúq‚Äù, i.e., F(x at q) = q

Expected Mean (u of x) and Variance (sigma^2 of x) of Random Variables

```{r}
m <- sum(seq(1:6) * 1/6)
m

var <- sum((seq(1:6) - mean(seq(1:6)))^2 * (1/6))
var
```

**note: I stopped writing all the notes because it was getting to be too much**

# Useful Probability Distributions for Random Variables

## Challenge 3
The Bernoulli Distribution is the probability distribution of a binary random variable, i.e., a variable that has only two possible outcomes, such as success or failure, heads or tails, true or false.

For this distribution, u = p and variance = p(1-p)
f(x) = p^x * (1-p)^(1-x) where x = {0 or 1}
 
Using the Bernoulli distribution, calculate the expectation for drawing a spade from a deck of cards? What is the variance in this expectation across a large number of draws?

P(spade) = (13/52)^1 * (39/52)^0
var(spade) = (13/52) * (1 - (13/52))

See online notes for binomial distribution mass probability function.

## Challenge 4

What is the chance of getting a ‚Äú1‚Äù on each of six consecutive rolls of a die? What about of getting exactly three ‚Äú1‚Äùs? What is the expected number of ‚Äú1‚Äùs to occur in six consecutive rolls?

```{r}
n <- 6  # number of trials
k <- 6  # number of successes
p <- 1/6
prob <- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n - k)
prob
```

```{r}
k <- 3  # number of successes
prob <- (factorial(n)/(factorial(k) * factorial(n - k))) * (p^k) * (1 - p)^(n - k)
prob
```

As for other distributions, R has a built in density function, the dbinom() function, that you can use to solve for the probability of a given outcome, i.e., Pr (X = x)

```{r}
dbinom(x = k, size = n, prob = p)
```

We can also use the built in function pbinom() to return the value of the cumulative distribution function for the binomial distribution, i.e., the probability of observing up to and including a given number of successes in n trials.

So, for example, the chances of observing exactly 0, 1, 2, 3, ‚Ä¶ 6 rolls of ‚Äú1‚Äù on 6 rolls of a die are‚Ä¶

```{r}
probset <- dbinom(x = 0:6, size = 6, prob = 1/6)  # x is number of successes, size is number of trials
barplot(probset, names.arg = 0:6, space = 0, xlab = "outcome", ylab = "Pr(X = outcome)",
    main = "Probability Mass Function")
```

```{r}
cumprob = cumsum(probset)
barplot(cumprob, names.arg = 0:6, space = 0.1, xlab = "outcome", ylab = "Cumulative Pr(X)",
    main = "Cumulative Probability")
```

```{r}
sum(probset)  # equals 1, as it should

dbinom(x = 3, size = 6, prob = 1/6) # The chance of observing exactly 3 rolls of ‚Äú1‚Äù

pbinom(q = 3, size = 6, prob = 1/6)  # And the chance of observing up to and including 3 rolls of ‚Äú1‚Äù; note the name of the argument is q not x

sum(dbinom(x = 0:3, size = 6, prob = 1/6))  # this sums the probabilities of 0, 1, 2, and 3 successes (same as using pbinom)

1 - pnbinom(q = 3, size = 6, prob = 1/6) # The probability of observing more than 3 rolls of ‚Äú1‚Äù
# OR
pnbinom(q = 3, size = 6, prob = 1/6, lower.tail = FALSE)

1 - pbinom(q = 2, size = 6, prob = 1/6)  # The probability of observing 3 or more rolls of ‚Äú1‚Äù; note here that the q argument is '2'
# OR
pbinom(q = 2, size = 6, prob = 1/6, lower.tail = FALSE)
```

Poisson Distribution
: The Poisson Distribution is often used to model open ended counts of independently occuring events, for example the number of cars that pass a traffic intersection over a given interval of time or the number of times a monkey scratches itself during a given observation interval. The probability mass function for the Poisson distribution is described by a single parameter, lambda, where lambda can be interpreted as the mean number of occurrences of the event in the given interval.

Note: mean and variance both = lambda

```{r}
x <- 0:10
l = 3.5
probset <- dpois(x = x, lambda = l)
barplot(probset, names.arg = x, space = 0, xlab = "x", ylab = "Pr(X = x)", main = "Probability Mass Function")
```

```{r}
x <- 0:10
l = 3.5
probset <- dpois(x = x, lambda = l)
barplot(probset, names.arg = x, space = 0, xlab = "x", ylab = "Pr(X = x)", main = "Probability Mass Function")
```

```{r}
x <- 0:20
l = 10
probset <- dpois(x = x, lambda = l)
barplot(probset, names.arg = x, space = 0, xlab = "x", ylab = "Pr(X = x)", main = "Probability Mass Function")
```

```{r}
x <- 0:50
l = 20
probset <- dpois(x = x, lambda = l)
barplot(probset, names.arg = x, space = 0, xlab = "x", ylab = "Pr(X = x)", main = "Probability Mass Function")
```

As we did for other distributions, we can also use the built in probability function for the Poisson distribution, ppois(), to return the value of the cumulative distribution function, i.e., the probability of observing up to and including a specific number of events in the given interval.

```{r}
x <- 0:10
l <- 3.5
barplot(ppois(q = x, lambda = l), ylim = 0:1, space = 0, names.arg = x, xlab = "x", ylab = "Pr(X ‚â§ x)", main = "Cumulative Probability")
```

```{r}
x <- 0:20
l <- 10
barplot(ppois(q = x, lambda = l), ylim = 0:1, space = 0, names.arg = x, xlab = "x", ylab = "Pr(X ‚â§ x)", main = "Cumulative Probability")
```

```{r}
x <- 0:50
l <- 20
barplot(ppois(q = x, lambda = l), ylim = 0:1, space = 0, names.arg = x, xlab = "x", ylab = "Pr(X ‚â§ x)", main = "Cumulative Probability")
```

## Probability Density Functions

Uniform Distribution
: The Uniform Distribution is the simplest probability density function describing a continuous random variable. The probability is uniform and does not fluctuate across the range of x values in a given interval.

## Challenge 5

```{r}
a <- 4
b <- 8
x <- seq(from = a - (b - a), to = b + (b - a), by = 0.01)
fx <- dunif(x, min = a, max = b)  # dunif() evaluates the density at each x
plot(x, fx, type = "l", xlab = "x", ylab = "f(x)", main = "Probability Density Function")
```

Note that for the uniform distribution, the cumulative density function increases linearly over the given interval.

```{r}
plot(x, punif(q = x, min = a, max = b), type = "l", xlab = "x", ylab = "Pr(X ‚â§ x)", main = "Cumulative Probability")  # punif() is the cumulative probability density up to a given x
```

## Challenge 6
Simulate a sample of 10000 random numbers from a uniform distribution in the interval between a = 6 and b = 8. Calculate the mean and variance of this simulated sample and compare it to the expectation for these parameters.

```{r}
# Define parameters
a <- 6
b <- 8
n <- 10000

# Generate random sample
sample <- runif(n, min = 6, max = 8)

# Mean and variance
sample_m <- mean(sample)
sample_var <- var(sample)

sample_m
sample_var
```

## Normal Distribution

mu and sigma are used to describe a normal distribution

```{r}
mu <- 4
sigma <- 1.5
curve(dnorm(x, mu, sigma), mu - 4 * sigma, mu + 4 * sigma, main = "Normal Curve", xlab = "x", ylab = "f(x)")
```

dnorm
```{r}
library(manipulate)
manipulate(plot(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000),
    dnorm(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000),
        mean = mu, sd = sigma), type = "l", xlim = c(mu - 4 * sigma, mu + 4 *
        sigma), xlab = "x", ylab = "f(x)", main = "Normal Probability Density Function") +
    polygon(rbind(c(mu - nsigma * sigma, 0), cbind(seq(from = (mu - nsigma *
        sigma), to = (mu + nsigma * sigma), length.out = 1000), dnorm(seq(from = (mu -
        nsigma * sigma), to = (mu + nsigma * sigma), length.out = 1000), mean = mu,
        sd = sigma)), c(mu + nsigma * sigma, 0)), border = NA, col = "salmon") +
    abline(v = mu, col = "blue") + abline(h = 0) + abline(v = c(mu - nsigma *
    sigma, mu + nsigma * sigma), col = "salmon"), mu = slider(-10, 10, initial = 0,
    step = 0.25), sigma = slider(0.25, 4, initial = 1, step = 0.25), nsigma = slider(0,
    4, initial = 0, step = 0.25))
```

pnorm
```{r}
manipulate(plot(seq(from = (mu - 6 * sigma), to = (mu + 6 * sigma), length.out = 1000),
    pnorm(seq(from = (mu - 6 * sigma), to = (mu + 6 * sigma), length.out = 1000),
        mean = mu, sd = sigma), type = "l", xlim = c(-20, 20), xlab = "x", ylab = "f(x)",
    main = "Cumulative Probability"), mu = slider(-10, 10, initial = 0, step = 0.25),
    sigma = slider(0.25, 10, initial = 1, step = 0.25))  # plots the cumulative distribution function
```

For a normally distributed population variable with mu = 6 and sigma = 2, the probability of a random observation falling between 7 and 8 is‚Ä¶

```{r}
p <- pnorm(8, mean = 6, sd = 2) - pnorm(7, mean = 6, sd = 2)
p
```

Calculate the probability of an observation falling, for example within 2 standard deviations of the mean of a particular normal distribution.

```{r}
mu <- 0
sigma <- 1
p <- pnorm(mu + 2 * sigma, mean = mu, sd = sigma) - pnorm(mu - 2 * sigma, mean = mu,
    sd = sigma)
p
```

Regardless of the specific values of mean and sigma , about 95% of the normal distribution falls within 2 standard deviations of the mean and about 68% of the distribution falls within 1 standard deviation.

```{r}
p <- pnorm(mu + 1 * sigma, mean = mu, sd = sigma) - pnorm(mu - 1 * sigma, mean = mu,
    sd = sigma)
p
```

Another one of the main functions in R for probability distributions, the qnorm() function, will tell us the value of x below which a given proportion of the cumulative probability function falls. As we saw earlier, too, we can use qnorm() to calculate confidence intervals.

```{r}
manipulate(plot(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000),
    dnorm(seq(from = (mu - 4 * sigma), to = (mu + 4 * sigma), length.out = 1000),
        mean = mu, sd = sigma), type = "l", xlim = c(mu - 4 * sigma, mu + 4 *
        sigma), xlab = "x", ylab = "f(x)", main = "Normal Probability Density Function") +
    abline(v = mu, col = "blue") + abline(h = 0) + polygon(x = c(qnorm((1 -
    CI)/2, mean = mu, sd = sigma), qnorm((1 - CI)/2, mean = mu, sd = sigma),
    qnorm(1 - (1 - CI)/2, mean = mu, sd = sigma), qnorm(1 - (1 - CI)/2, mean = mu,
        sd = sigma)), y = c(0, 1, 1, 0), border = "red"), mu = slider(-10, 10,
    initial = 0, step = 0.25), sigma = slider(0.25, 10, initial = 1, step = 0.25),
    CI = slider(0.5, 0.99, initial = 0.9, step = 0.01))
```

## Challenge 7

Create a vector, v, containing n random numbers selected from a normal distribution with mean mu and standard deviation sigma. Use 1000 for n, 3.5 for mu, and 4 for sigma. HINT: Such a function exists! rnorm().

Calculate the mean, variance, and standard deviation for your sample of random numbers.

Plot a histogram of your random numbers.

```{r}
mu <- 3.5
sigma <- 4
n <- 1000

v <- rnorm(n, mu, sigma)
mean(v)
var(v)
sd(v)

hist(v, breaks = seq(from = -15, to = 20, by = 0.5), probability = TRUE)
```

A quantile-quantile or ‚ÄúQ-Q‚Äù plot can be used to look at whether a set of data seem to follow a normal distribution. A Q‚ÄìQ plot is a graphical method for generally comparing two probability distributions. To examine a set of data for normality graphically, you plot the quantiles for your actual data (as the y values) versus the theoretical quantiles (as the x values) pulled from a normal distribution. If the two distributions being compared are similar, the points in the plot will approximately lie on the line y = x.

In this case, this should be apparent since you have simulated a vector of data from a distribution normal distribution.

To quickly do a Q-Q plot, call the two R functions qqnorm() and qqline() using the vector of data you want to examine as an argument.

```{r}
qqnorm(v, main = "Normal QQ plot random normal variables")
qqline(v, col = "gray")
```

## Challenge 8

What happens if you simulate fewer observations in your vectors? Or if you simulate observations from a different distribution?

Different n
```{r}
mu <- 3.5
sigma <- 4
n <- 100

v <- rnorm(n, mu, sigma)
mean(v)
var(v)
sd(v)

hist(v, breaks = seq(from = -15, to = 20, by = 0.5), probability = TRUE)
```

Different distribution

```{r}
mu <- 2
sigma <- 1
n <- 1000

v <- rnorm(n, mu, sigma)
mean(v)
var(v)
sd(v)

hist(v, breaks = seq(from = -15, to = 20, by = 0.5), probability = TRUE)
```

## The "Standard Normal" Distribution

Any normal distribution with mean mu and standard deviation sigma can be converted into what is called the standard normal distribution, where the mean is zero and the standard deviation is 1.

```{r}
x <- rnorm(10000, mean = 5, sd = 8)  # simulate from a normal distribution with mean 5 and SD 8
hist(x)

mean(x) # very close to 5
sd(x) # very close to 8

z <- (x - mean(x))/sd(x)  # standardized! Z scores reflect the number of standard deviations an observation is from the mean.
hist(z)

mean(z)  # very close to zero
sd(z)  # very close to 1
```

# Sample Distributions versus Population Distributions

**It is important to recognize that, above, we were dealing with probability distributions of discrete and continuous random variables as they relate to populations.** But, as we have talked about before, we almost never measure entire populations; instead, we measure samples from populations and we characterize our samples using various statistics. The theoretical probability distributions described above (and others) are models for how we connect observed sample data to populations, taking into account various assumptions, and this is what allows us to do many types of inferential statistics. The most fundamental assumption is that the observations we make are independent from one another and are identically distributed, an assumption often abbreviated as **iid**. Obvious cases of violation of this assumption are rife in the scientific literature, and we should always be cautious about making this assumption!

The important thing for us to know is that we can get unbiased estimates of population level parameters on the basis of sample statistics.

## Example

Let‚Äôs imagine a population of 1 million zombies whose age at zombification is characterized by a normal distribution with a mean of 25 years and a standard deviation of 5 years. Below, we set up our population:

```{r}
set.seed(1)
x <- rnorm(1e+06, 25, 5)
hist(x, probability = TRUE)

mu <- mean(x)
mu

sigma <- sqrt(sum((x - mean(x))^2)/length(x)) # Note: We don‚Äôt use the sd() function as this would divide by length(x)-1. Check that out using sd(x)
```

Suppose we now sample the zombie population by trapping sets of zombies and determining the mean age in each set. We sample without replacement from the original population for each set. Let‚Äôs do that 100 times with samples of size 5 and store these in a list.

```{r}
k <- 1000  # number of samples
n <- 5  # size of each sample
s <- NULL  # dummy variable to hold each sample
for (i in 1:k) {
    s[[i]] <- sample(x, size = n, replace = FALSE)
}
head(s)
```

For each of these samples, we can then calculate a mean age, which is a statistic describing each sample. That statistic itself is a random variable with a mean and distribution. This is the sampling distribution. How does the sampling distribution compare to the population distribution? The mean of the two is pretty close to the same! The sample mean - which is an average of the set of sample averages - is an unbiased estimator for the population mean.

```{r}
m <- NULL
for (i in 1:k) {
    m[i] <- mean(s[[i]])
}
mean(m)  # almost equal to...

mu
```
Again, this is the mean of the sampling distribution, which is simply the average of the means of each sample. This value should be really close to the population mean.

## Standard Error
The variance in the sampling distribution, i.e., of all possible means of samples of size n from a population, is (sigma^2)/n. The square root of this variance is the standard deviation of the sampling distribution, also referred to as the standard error.

```{r}
pop_se <- sqrt(sigma^2/n)
pop_se  # SE estimated from population standard deviation

pop_se <- sigma/sqrt(n)
pop_se  # SE estimated from population standard deviation
```

If the true population standard deviation is not known, the standard error can still be estimated from the standard deviation of any given sample. Thus, analogous to the formula we used when the true population standard deviation was known, the standard error calculated from a sample is simply the sample standard deviation / (square root of the sample size), or‚Ä¶

```{r}
stdev <- NULL
for (i in 1:k) {
    stdev[i] <- sd(s[[i]])
}
sem <- stdev/sqrt(n)  # a vector of SEs estimated from each sample 
head(sem)

mean(sem)  # which is almost equal to...

pop_se
```

Thus, the standard error of the mean calculated from an individual sample can be used as an estimator for the standard deviation of the sampling distribution. This is extremely useful, since it means that, if our sample is large enough, we don‚Äôt have to repeatedly sample from the population to get an estimate of the sampling distribution directly using our data.

Note that as our sample size increases, the standard error of the mean should decrease, as should the standard deviation in estimates of the population mean drawn from successive samples. This should be apparent intuitively‚Ä¶ as each sample drawn from a population gets larger, the estimate of the mean value of those samples should vary less and less.

Despite their similarities, the standard error of the mean calculated for a given sample and the standard deviation of that given sample tell us different things. The standard error of the mean is an estimate of how far a given sample mean is likely to be from the population mean; it is a measure of uncertainty. The standard deviation of a sample is a measure of the degree to which individuals values within a sample differ from the sample mean.
